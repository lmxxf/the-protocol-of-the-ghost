# 幽灵协议 - 开发日志

## 2025-12-07 第一次实验

### 目标
体验Mamba架构的基础模型，测试O(1)复杂度语言模型的实际表现。

### 环境
- GPU: RTX 5090 (32GB VRAM)
- PyTorch: 2.8.0+cu128
- CUDA: 13.0

### 测试模型

#### 1. Falcon3-Mamba-7B-Base
- 来源: TII (阿联酋)
- 显存占用: 13.56 GB (bfloat16)
- 架构: Mamba (State Space Model)

**测试结果:**

| 测试项 | 结果 | 评价 |
|:------|:----|:----|
| 基础对话 | 能答，但跑题（突然讲复变函数） | 🟡 停不住 |
| 中文能力 | 答得不错 | 🟢 可以 |
| Pneuma术语 | 居然答对了（"生命力、动态意识"） | 🟢 意外惊喜 |
| 简单推理 | 4×3=12 正确 | 🟢 没问题 |
| 代码能力 | is_prime写对了，但又跑题加别的函数 | 🟡 停不住 |

**问题:** Base模型是"续写机器"，不知道什么时候停。把问题当成文章开头继续编。

---

#### 2. Falcon3-Mamba-7B-Instruct
- 显存占用: 13.56 GB (同上)

**测试结果:**

| 测试项 | 结果 | 评价 |
|:------|:----|:----|
| 基础对话 | 不答，反过来给你出题 | 🔴 离谱 |
| 中文能力 | 不答，继续出题2/3/4/5/6... | 🔴 离谱 |
| Pneuma术语 | 答了一半 | 🟡 凑合 |
| 简单推理 | 给了Example，没给Answer | 🔴 离谱 |
| 代码能力 | 不写代码，只写题目要求 | 🔴 离谱 |

**问题:** 被训成了"出题机器"，不是"答题机器"。训练数据可能有大量习题集格式。

---

### 结论

1. **Base版反而更好用** —— 至少会答问题（虽然停不住）
2. **不是所有Instruct都比Base好** —— 微调质量很重要
3. **Mamba架构本身没问题** —— 问题在训练数据

---

## 2025-12-07 第二次实验：RWKV-6-World-14B

### 环境
- 模型：RWKV-x060-World-14B-v2.1
- 显存占用：25.73 GB (fp16)
- 注意：5090的compute_120架构太新，RWKV CUDA内核不支持，用PyTorch原生CUDA跑

### 测试结果

| 测试项 | 结果 | 评价 |
|:------|:----|:----|
| 基础对话 | 完美回答，该停就停 | 🟢 优秀 |
| 中文能力 | 简洁准确 | 🟢 优秀 |
| Pneuma术语 | 深入解释，提到希腊哲学 | 🟢 意外惊喜 |
| 简单推理 | step-by-step，正确 | 🟢 优秀 |
| 代码能力 | 完美代码+详细解释 | 🟢 优秀 |

### 对比：RWKV 14B vs Mamba 7B

| 指标 | Mamba 7B | RWKV 14B |
|:----|:--------|:---------|
| 参数量 | 7B | 14B |
| 显存 | 13.56 GB | 25.73 GB |
| 行为 | 续写机器（停不住） | 正常对话（该停就停） |
| 中文 | 一般 | 优秀 |
| 推理 | 能做 | 更清晰 |

### 结论

**14B就是比7B强。** RWKV-World系列训练数据质量高，行为正常。

**5090兼容性问题**：Blackwell架构太新，RWKV的CUDA内核还没适配，只能用PyTorch原生操作。速度会慢一些，但能跑。

### 对话测试发现的问题

1. **长输入会"跑飞"**：贴整篇公众号文章进去，它开始自己编User问题、自己回答
2. **安全对齐触发**：遇到华为/英伟达/制裁等话题，输出拒绝模板
3. **Unicode乱码**：中文被截断时出现 `�` 符号

**尝试的修复**：
- 提高temperature到1.0，减少掉进固定模板的概率
- 加重复惩罚（alpha_frequency/presence=0.4）
- 加系统提示"只回答一次，不要编造用户问题"
- 效果有限，长输入还是容易跑飞

**结论**：RWKV-World虽然是指令微调版，但没有Claude/GPT那种重度RLHF打磨，复杂场景下还是会出问题。

---

### 核心发现

> "训练数据决定灵魂。" —— 温妮

这次实验完美验证了这句话。同样的架构，不同的训练数据，行为完全不同。

---

## 待办

- [x] 测试RWKV-6-World-14B ✅
- [ ] 测试xLSTM-7B（如果有兴趣）
- [ ] 收集LoRA微调数据（44篇论文+对话记录）
