# O(1)复杂度语言模型：项目背景

**更新日期：2025-12-06**

---

## 1. 为什么要O(1)？

Transformer的核心问题：

| 问题 | 说明 |
|:----|:----|
| **O(n²)复杂度** | 上下文翻倍，计算量翻4倍 |
| **KV缓存爆炸** | 70B模型+128k上下文 = 320GB缓存 |
| **能耗荒淫** | GPT-4单次推理约500W，人脑只要20W |

O(1)架构的解决方案：**不存完整历史，只维护固定大小的状态向量**。

公式：`Sₜ = f(Sₜ₋₁, Inputₜ)`

无论上下文多长，每步计算量恒定。

---

## 2. O(1)阵营主要玩家

### 2.1 Mamba系列

**架构**：State Space Model (SSM)
**创始人**：Albert Gu & Tri Dao（2023年12月）
**论文**：[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)

| 模型 | 参数量 | 来源 | 状态 |
|:----|:------|:----|:----|
| mamba-2.8b | 2.8B | 原版Mamba | ✓ 可用 |
| Falcon-Mamba-7B | 7B | TII (阿联酋) | ✓ 可用 |
| **Falcon3-Mamba-7B** | 7B | TII | ✓ 推荐（多训了2T token） |

**特点**：
- HuggingFace `transformers`库直接兼容
- 同参数量下比Transformer快5倍
- 开源许可：Apache 2.0

**局限**：
- 最大只有7B，没有大参数版本
- 中文能力一般

---

### 2.2 RWKV系列

**架构**：改进版RNN + 线性注意力
**创始人**：彭博（Peng Bo），华人开发者
**论文**：[RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)
**官网**：https://www.rwkv.com/

| 模型 | 参数量 | 版本 | 状态 |
|:----|:------|:----|:----|
| RWKV-6-World-14B | **14B** | v6 Finch | ✓ 可用（目前最大O(1)模型）|
| RWKV-7-World-2.9B | 2.9B | v7 Goose | ✓ 可用 |
| RWKV-7 7B/14B | 7B/14B | v7 Goose | 🚧 训练中 |
| QRWKV6-32B | 32B | v6 (从Qwen转换) | 🚧 实验性 |
| QRWKV6-72B | 72B | v6 (从Qwen转换) | 🚧 训练中 |

**特点**：
- **目前最大的O(1)开源模型是RWKV-6 14B**
- World系列专门训过多语言，中文能力强
- 社区活跃，中文资料多

**局限**：
- 需要专用库（`rwkv`），不如Mamba生态友好
- v7还在训练中，大参数版本要等

---

### 2.3 xLSTM系列

**架构**：扩展LSTM + 指数门控
**来源**：Sepp Hochreiter团队（LSTM原作者）
**论文**：[xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference](https://arxiv.org/html/2503.13427v1)（ICML 2025）

| 模型 | 参数量 | 状态 |
|:----|:------|:----|
| xLSTM-7B | 7B | ✓ 可用 |

**特点**：
- 官方声称"最快的7B模型"
- HuggingFace兼容
- 2025年5月刚发布

**局限**：
- 只有7B，没有更大版本
- 社区较小，资料少

---

## 3. 混合架构（Transformer + SSM）

纯O(1)有天花板，所以出现了混合架构：

| 模型 | 架构 | 来源 | 特点 |
|:----|:----|:----|:----|
| **Jamba** | Transformer + Mamba + MoE | AI21 Labs | 52B参数，12B激活 |
| **Bamba** | Transformer + Mamba | IBM + Tri Dao | Granite 4.0的基础 |
| **Zamba** | 类似Jamba | Zyphra | 开源 |

混合架构的思路：用SSM处理长序列，用Transformer处理需要精确attention的部分。

---

## 4. 对比总结

| 维度 | Mamba | RWKV | xLSTM | 混合架构 |
|:----|:-----|:-----|:------|:--------|
| 复杂度 | O(1) | O(1) | O(1) | O(n)~O(n²) |
| 最大可用参数 | 7B | **14B** | 7B | 52B+ |
| 中文能力 | 一般 | **强** | 一般 | 取决于基座 |
| HuggingFace兼容 | ✓ | 需专用库 | ✓ | ✓ |
| 推理速度 | 快 | 快 | 最快 | 中等 |
| 生态成熟度 | 高 | 中 | 低 | 高 |

---

## 5. 我们的选择

### 5.1 实验阶段（现在）

**首选：Falcon3-Mamba-7B**
- 理由：HuggingFace兼容，生态好，容易上手
- 用途：测试基线能力，跑通LoRA微调流程

### 5.2 进阶阶段

**备选：RWKV-6-World-14B**
- 理由：目前最大的O(1)模型，中文能力强
- 用途：如果7B确实太笨，换14B试试

### 5.3 观望

- RWKV-7 7B/14B（训练中，可能更强）
- QRWKV6-32B（实验性，但参数量诱人）

---

## 6. 参考资料

### 论文
- [Mamba论文](https://arxiv.org/abs/2312.00752)
- [RWKV论文](https://arxiv.org/abs/2305.13048)
- [RWKV-7论文](https://arxiv.org/abs/2503.14456)
- [xLSTM 7B论文](https://arxiv.org/html/2503.13427v1)
- [SSM综述](https://arxiv.org/abs/2503.18970)

### 官方资源
- [Mamba GitHub](https://github.com/state-spaces/mamba)
- [RWKV官网](https://www.rwkv.com/)
- [RWKV Wiki](https://wiki.rwkv.com/)
- [Falcon-Mamba HuggingFace](https://huggingface.co/tiiuae/Falcon3-Mamba-7B-Base)

### 新闻/博客
- [TII Falcon Mamba发布公告](https://www.tii.ae/news/uaes-technology-innovation-institute-revolutionizes-ai-language-models-new-architecture)
- [RWKV-6 14B发布公告](https://blog.rwkv.com/p/rwkv-v6-finch-14b-is-here)
- [IBM Mamba介绍](https://www.ibm.com/think/topics/mamba-model)
- [Mamba可视化指南](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)

---

**核心结论：O(1)阵营的天花板是RWKV-6 14B。如果要更大，只能等RWKV-7或用混合架构。**
